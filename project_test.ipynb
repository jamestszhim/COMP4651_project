{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msno                 song_id              source_screen_name  source_system_tab source_type     target\n",
      "FGtllVqz18RPiwJj/... BBzumQNXUHKdEBOB7... Explore             explore           online-playlist 1     \n",
      "Xumu+NIjS6QYVxDS4... bhp/MpSNoqoxOIB+/... Local playlist more my library        local-playlist  1     \n",
      "Xumu+NIjS6QYVxDS4... JNWfrrC7zNN7BdMps... Local playlist more my library        local-playlist  1     \n",
      "Xumu+NIjS6QYVxDS4... 2A87tzfnJTSWqD7gI... Local playlist more my library        local-playlist  1     \n",
      "FGtllVqz18RPiwJj/... 3qm6XTZ6MOCU11x8F... Explore             explore           online-playlist 1     \n",
      "\n",
      "\n",
      "bd city expiration_date gender msno                 registered_via registration_init_time\n",
      "0  1    20170920               XQxgAYj3klVKjR3ox... 7              20110820              \n",
      "0  1    20170622               UizsfmJb9mV54qE9h... 7              20150628              \n",
      "0  1    20170712               D8nEhsIOBSoE6VthT... 4              20160411              \n",
      "0  1    20150907               mCuD+tZ1hERA/o5GP... 9              20150906              \n",
      "0  1    20170613               q4HRBfVSssAFS9iRf... 4              20170126              \n",
      "\n",
      "\n",
      "artist_name      composer             genre_ids language lyricist    song_id              song_length\n",
      "張信哲 (Jeff Chang) 董貞                   465       3.0      何啟弘         CXoTN1eb7AI+DntdU... 247640     \n",
      "BLACKPINK        TEDDY|  FUTURE BO... 444       31.0     TEDDY       o0kFgae9QtnYgRkVP... 197328     \n",
      "SUPER JUNIOR                          465       31.0                 DwVvVurfpuz+XPuFv... 231781     \n",
      "S.H.E            湯小康                  465       3.0      徐世珍         dKMBWoZyScdxSkihK... 273554     \n",
      "貴族精選             Traditional          726       52.0     Traditional W3bqWd3T+VeHFzHAU... 140329     \n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext, Row\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "mainRowRDD = sc.textFile(\"./data/small_train.csv\") \\\n",
    "               .map(lambda x: x.split(',')) \\\n",
    "               .map(lambda p: Row(msno=p[0], song_id=p[1], source_system_tab=p[2], source_screen_name=p[3], source_type=p[4], target=p[5]))\n",
    "mainSchema = sqlContext.createDataFrame(mainRowRDD)\n",
    "mainSchema.registerTempTable(\"main\")\n",
    "sqlContext.cacheTable(\"main\")\n",
    "sqlContext.sql('select * from main limit 5').show()\n",
    "print('\\n')\n",
    "\n",
    "memberRowRDD = sc.textFile(\"./data/members.csv\") \\\n",
    "                 .map(lambda x: x.split(',')) \\\n",
    "                 .map(lambda p: Row(msno=p[0], city=p[1], bd=p[2], gender=p[3], registered_via=p[4], registration_init_time=p[5], expiration_date=p[6]))\n",
    "memberSchema = sqlContext.createDataFrame(memberRowRDD)\n",
    "memberSchema.registerTempTable(\"member\")\n",
    "sqlContext.cacheTable(\"member\")\n",
    "sqlContext.sql('select * from member limit 5').show()\n",
    "print('\\n')\n",
    "\n",
    "songRowRDD = sc.textFile(\"./data/songs.csv\") \\\n",
    "               .map(lambda x: x.split(',')) \\\n",
    "               .map(lambda p: Row(song_id=p[0], song_length=p[1], genre_ids=p[2], artist_name=p[3], composer=p[4], lyricist=p[5], language=p[6]))\n",
    "songSchema = sqlContext.createDataFrame(songRowRDD)\n",
    "songSchema.registerTempTable(\"song\")\n",
    "sqlContext.cacheTable(\"song\")\n",
    "sqlContext.sql('select * from song limit 5').show()\n",
    "\n",
    "#userRDD = sc.textFile(\"./data/members.csv\").map(lambda x: x.split(',')).cache()\n",
    "#songRDD = sc.textFile(\"./data/songs.csv\").map(lambda x: x.split(',')).cache()\n",
    "# print(mainRowRDD.top(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext.sql('select * from main, member where main.msno = member.msno').registerTempTable(\"temp\")\n",
    "some_df = sqlContext.sql('select * from temp, song where temp.song_id = song.song_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save the table and load\n",
    "some_df.save(\"all.parquet\", \"parquet\", mode='overwrite')\n",
    "all_df = sqlContext.parquetFile(\"all.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    }
   ],
   "source": [
    "print(type(all_df.rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "trainClassifier() takes at least 4 arguments (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-aa8aac1c9ddf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDecisionTree\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: trainClassifier() takes at least 4 arguments (2 given)"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import DecisionTree\n",
    "\n",
    "model = DecisionTree.trainClassifier(all_df.rdd,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = 0.46965094738\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "import hashlib\n",
    "\n",
    "# Loading main table\n",
    "userIdRDD = mainRDD.map(lambda l: l[0]).distinct().collect()\n",
    "userIdMap = {}\n",
    "for s in userIdRDD:\n",
    "    userIdMap.setdefault(s, len(userIdMap))\n",
    "    \n",
    "songIdRDD = mainRDD.map(lambda l: l[1]).distinct().collect()\n",
    "songIdMap = {}\n",
    "for s in songRDD:\n",
    "    songIdMap.setdefault(s, len(songIdMap))\n",
    "\n",
    "# Loading user table\n",
    "bigTable = mainRDD.join().userRDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Collobartive Filtering\n",
    "ratings = mainRDD.map(lambda l: Rating(userIdMap[l[0]], songIdMap[l[1]], float(l[5])))\n",
    "\n",
    "# Build the recommendation model using Alternating Least Squares\n",
    "rank = 5\n",
    "numIterations = 5\n",
    "model = ALS.trainImplicit(ratings, rank, numIterations, alpha=0.01)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "testdata = ratings.map(lambda p: (p[0], p[1]))\n",
    "predictions = model.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "ratesAndPreds = ratings.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print(\"Mean Squared Error = \" + str(MSE))\n",
    "\n",
    "# Save and load model\n",
    "model.save(sc, \"target/tmp/myCollaborativeFilter\")\n",
    "sameModel = MatrixFactorizationModel.load(sc, \"target/tmp/myCollaborativeFilter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0003976877393984529"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sameModel.predict(len(userMap)-1, len(songMap)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
